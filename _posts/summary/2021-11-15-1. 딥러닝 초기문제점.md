---

author: Hone Ye ji
categories: 
 - 딥러닝
tags: 
 - 딥러닝

toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
---

수업들으면서 요점정리했던 것을 기록한다.

# 딥러닝 초기 문제점

 딥러닝 초기 문제점에 대한 설명입니다. 특징으로는 vanishing gradient(기울기 소실), weight initialization(가중치 초기화), overfitting(오버피팅) 이 있습니다.

## 1) vanishing gradient (기울기 소실) 

- 기울기소실이란? 학습이 진행되면서 각 파라미터에 대한 가중치의 미분값(경사)가 매우 작아져 0에 가깝게 되는 현상
- 가장 큰 원인은 시그모이드 함수다. 0과 1사이의 값을 가지는 방식으로 여러번 지속된다면 0에 수렴하기때문에 backpropagation이 정상적으로 작동하지않는다. 또한, 평균이 0이 아니라 0.5이며, 양수를 출력하여 가중치합이 입력의 가중치 합보다 커질 가능성이 높다. 이런 이유로 각 레이어를 지날때 분산이 커져 가장 높은 레이어에서는 출력이 0 또는 1로 수렴되어 기울기 소실이 발생한다.
![시그모이드 함수(Sigmoid function)](https://t1.daumcdn.net/cfile/tistory/99FC323D5DA6F5251D)
- 활성화 함수를 시그모이드 함수가 아닌 다양한 활성화 함수를 적용하는 것이 필요하다. tanh, ReLU, Leacky ReLU 등이 있다. 예를들어,  ReLU  max(0,x)를 이용하게 되면 값이 줄어드는 것을 방지할 수 있다. 
- 역전파시키면 gradient값이 input방향으로 진행될 수록 미약해진다. 그러므로 앞쪽 노드들에 대한 weight가 영향을 받지 못한다.


## 2) weight initialization(가중치 초기화)
- weight를 0으로 두면 학습이 이루어지지않는 경우가 발생한다.
	- RBM: forward, backward 계산 후, 최소 x값,  $\widehat{x}$ 값 비교하여 최적의 초기값(w,b)설정  -> fine tuning, pre-training step
	- Xaiver: 이전 노드와 다음 노드의 개수에 의존하는 방법
				$입력값~출력값 사이 임의의 수 \over 출력/입력값^2$ $\Longrightarrow$  $입력값~출력값 사이 임의의 수 \over 출력/(입력값/2)^2$ 
	- sigmoid, tanh $\to$ Xavier,ReLU $\to$ He 
	

## 3) overfitting 
- **오버피팅?** train 데이터에 너무 최적화되어 만들어진 모델, 즉 train데이터에 너무 정확하게 학습이 되어 train 결과에는 100%에 가까운 정확도를 가지나, test데이터로 평가하면 정확도가 급격하게 떨러진다.  
- 밑 그래프에서 오른쪽 부분, generalization loss는 일반화를 할 수 있는 지 평가하는 것으로, test 데이터로 평가했다고 가정할 수 있다. 두 선(training과 generalization)의 차이가 많이 나므로 이를 오버피팅이 되었다고 할 수 있다. 
- **언더피팅?** 학습 데이터가 충분하지않거나 학습이 제대로 되지 않아서, train 데이터에 가깝게 가지 못한 경우를 말한다. 위 그래프에서 초반에 loss가 높은 부분을 말한다. 

	![3.11. 모델 선택, 언더피팅(underfitting), 오버피팅(overfitting) — Dive into Deep  Learning documentation](https://ko.d2l.ai/_images/capacity_vs_error.svg)

	- Training Error :학습데이터에서의 에러
	- Generalization Error: 새로운 데이터의 모델에러 (모델 생성에 참여안한 데이터의 오류)
	- 이때, Generalization error는 커지고 Training error는 작아지며 사이의 차이가 많이 나게 되면 오버피팅(overfitting)되었다고 판단한다.
- 모델복잡도가 높을 수록  Generalization error과 Training error의 차이는 커진다.

- **underfitting vs overfitting 비교**

|모델복잡도(capacity)  |  |
|--|--|--|--|
| **낮음** | **underfitting** |
| **높음**| **overfitting** |

- capacity (다양한 함수로 변형될 수 있는 능력)
	-  낮은 경우: underfitting
		-  미적합된 경우로, 학습데이터 미진하게 학습, 모델단순, 데이터 많음
	-  높은 경우: overfitting -> 어떠한 형태로든 변형될 수 있음
		- 과적합된 경우로, 학습데이터 지나치게 적합, 모델 복잡(파라미터수 많음, 가중치 넓은 범위의 값), 학습데이터수가 적음
	- 학습데이터 추가확보가 어렵기때문에 모델 복잡도를 조정(차수조정)

-  **overfitting 방지 (test error를 줄이기 위해 수정)**
	1) 데이터 수 늘리기
	2) feature 수 줄이기
	3) dropout 사용
	4) regulation
	5) batch normalization  

- **해결방법**
	- 정규화 : cost + $\lambda\sum w^2$ 
		- w값이 튀지않게 제어
	- dropout: 학습은 0.5~0.7로 테스트는 100%사용
<!--stackedit_data:
eyJoaXN0b3J5IjpbODY4MTU3NTQ4LDIwMTg5NzgxMzZdfQ==
-->