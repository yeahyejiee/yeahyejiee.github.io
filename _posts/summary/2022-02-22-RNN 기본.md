
	
# REVIEW
- ANN: 사람의 신경망 원리와 구조 모방하여 만든 기계학습 알고리즘 (인공신경망)
- DNN: ANN기법에서 모델 내 은닉층을 많이 늘림 (은닉층 2개이상)
- CNN : 합성곱 계층을 통해 데이터의 특징을 추출한 후, 그 특징들을 기반으로 분류하는 딥러닝 분석 모델
- RNN: 데이터 순서 정보를 반영하는 재귀구조의 딥러닝 분석 모델, 히든 layer 순환구조

# RNN(Recurrent Neural Networks)
### 1) RNN
-  시퀀스 모델 (자연이나 음성신호, 주식과 같은 연속적인 시계열 데이터에 적합한 모델)
- 히든 layer에서도 서로 값을 주고 받음
- 적용(문서분류, 시간분류, 시계열, 번역기, 감성분석, 날씨예측)
### 2) 공식
$h_t = h_w(h_{t-1}, x_t)$
$h_t=tanh(W_{hh}h_{t-1}+W_{xh}X_t)$
$y_t=W_{hy}h_{t}$


![image](https://user-images.githubusercontent.com/45659433/155084101-3681abc0-600e-487f-a420-256a24ab836b.png)

### 3) character-level language model
- input 단어: one-hot-encoding
- 단어 $10^7$일경우, $10^4$개이하로
- stopwords 제거 
	- entropy : binary-cross entropy, cross entropy
	- DF(document frequency) :특정 단어가 등장한 문서의 수
	- TF(term frequency): 특정 문서에서의 특정 단어의 등장 횟수
		![image](https://user-images.githubusercontent.com/45659433/155087467-93088e9b-cb04-40e4-83e2-8d6f5b96e6bf.png)
- feature selection (어떤 단어가 유용한지)
	- a=10, b=100, c=5
	- zipf's power law 
		- 반비례, 불용어 포함 in b, 순위 낮은건 제외
		![image](https://user-images.githubusercontent.com/45659433/155089851-1772b65f-a7f4-4eee-94db-946b8742e716.png)
-gradient clipping : 미분 값이 커지지 않도록 잘라냄
$g\leftarrow min(1,\frac{\seta}{\| g\|}$
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTUzMjIyNzQ2MiwxMzc4MjY0MjIwLC0xNj
k0MzkzOTIsODExNDM5OTUwLDU5NjM3NzQ3NSwtMTUyMDAzMTMz
MiwtMTgwNzEzOTY0MSw1Nzk5NDg0MDUsMTQ3MzU2MTkwMywtMj
AxNzAzMzAzOF19
-->