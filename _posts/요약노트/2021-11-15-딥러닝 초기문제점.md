
---
author: Hone Ye ji
categories: 
 - 딥러닝
 
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"

---

수업들으면서 요점정리했던 것을 기록한다.

# 1. linear regression

 딥러닝 초기 문제점에 대한 설명입니다. 특징으로는 vanishing gradient(기울기 소실), weight initialization(가중치 초기화), overfitting 피하기가 있습니다.

## 1) vanishing gradient (기울기 소실) 

- 기울기소실이란? 학습이 진행되면서 각 파라미터에 대한 가중치의 미분값(경사)가 매우 작아져 0에 가깝게 되는 현상
- 가장 큰 원인은 시그모이드 함수다. 0과 1사이의 값을 가지는 방식으로 여러번 지속된다면 0에 수렴하기때문에 backpropagation이 정상적으로 작동하지않는다. 또한, 평균이 0이 아니라 0.5이며, 양수를 출력하여 가중치합이 입력의 가중치 합보다 커질 가능성이 높다. 이런 이유로 각 레이어를 지날때 분산이 커져 가장 높은 레이어에서는 출력이 0 또는 1로 수렴되어 기울기 소실이 발생한다.
![시그모이드 함수(Sigmoid function)](https://t1.daumcdn.net/cfile/tistory/99FC323D5DA6F5251D)
- 활성화 함수를 시그모이드 함수가 아닌 다양한 활성화 함수를 적용하는 것이 필요하다. tanh, ReLU, Leacky ReLU 등이 있다. 예를들어,  ReLU  max(0,x)를 이용하게 되면 값이 줄어드는 것을 방지할 수 있다. 
- 역전파시키면 gradient값이 input방향으로 진행될 수록 미약해진다. 그러므로 앞쪽 노드들에 대한 weight가 영향을 받지 못한다.


## 2) weight initialization(가중치 초기화)
- weight를 0으로 두면 학습이 이루어지지않는 경우가 발생한다.
	- RBM: forward, backward 계산 후, 최소 x값  $$\widehat{x} $$ 값 비교하여 최적의 초기값(w,b)설정  -> fine tuning,pre-training step
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTIwMzMzMDk3LDYzNzEzMTI4OSwtMTUxNT
cwODUxNl19
-->